# Install required packages if not already installed
import subprocess
import sys

def install_package(package):
    try:
        __import__(package)
    except ImportError:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package, "-q"])

# Install required packages
install_package("torch")
install_package("torchvision")
install_package("scikit-learn")
install_package("seaborn")
install_package("tqdm")
install_package("Pillow")

import os
import torch
import torchvision
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import re
from collections import Counter
from PIL import Image

from torchvision import datasets, transforms
from torch import nn, optim
from torch.utils.data import DataLoader, Dataset
from torch.utils.data import random_split

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# Check if CUDA is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"CUDA Version: {torch.version.cuda}")



# Quick Test - Verify Setup
print("Testing setup...")
print(f"Current directory: {os.getcwd()}")
print(f"Images folder exists: {os.path.exists('images')}")

if os.path.exists('images'):
    image_files = [f for f in os.listdir('images') if f.endswith('.jpg')]
    print(f"Number of images found: {len(image_files)}")
    if len(image_files) > 0:
        print(f"Sample image: {image_files[0]}")
        print("✓ Setup looks good! You can proceed to the next cells.")
    else:
        print("⚠ No .jpg files found in images folder!")
else:
    print("⚠ Images folder not found! Make sure the 'images' folder is in the same directory as this notebook.")


# Dataset Information
print("=" * 80)
print("OXFORD-IIIT PET DATASET CLASSIFICATION PROJECT")
print("=" * 80)
print("\nDataset: Oxford-IIIT Pet Dataset")
print("Source: https://www.kaggle.com/datasets/tanlikesmath/the-oxfordiiit-pet-dataset")
print("Original Source: Visual Geometry Group, University of Oxford")
print("\nDataset Statistics:")
print(f"- Total images: 14,779")
print(f"- Dataset size: ~1.5 GB")
print(f"- Number of classes: 37 (25 dog breeds + 12 cat breeds)")
print(f"- Images per class: ~200")
print("\nTask: Multi-class pet breed classification")
print("=" * 80)


# Custom Dataset Class for Oxford-IIIT Pet Dataset
class PetDataset(Dataset):
    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        image = Image.open(image_path).convert('RGB')
        label = self.labels[idx]
        
        if self.transform:
            image = self.transform(image)
        
        return image, label

# Function to extract breed from filename
def extract_breed(filename):
    # Filename format: "breed_number.jpg"
    # Extract breed name (everything before the last underscore)
    match = re.match(r'(.+?)_\d+\.jpg', filename)
    if match:
        return match.group(1)
    return None

# Load all image paths and labels
print("Loading dataset...")
image_dir = "images"
all_images = []
all_labels = []
breed_to_idx = {}

# Get all jpg files
image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]
print(f"Found {len(image_files)} images")

# Extract breeds and create label mapping
breeds = []
for img_file in image_files:
    breed = extract_breed(img_file)
    if breed:
        breeds.append(breed)

# Create breed to index mapping
unique_breeds = sorted(set(breeds))
breed_to_idx = {breed: idx for idx, breed in enumerate(unique_breeds)}
idx_to_breed = {idx: breed for breed, idx in breed_to_idx.items()}

print(f"\nFound {len(unique_breeds)} unique breeds:")
for i, breed in enumerate(unique_breeds):
    count = breeds.count(breed)
    print(f"  {i+1:2d}. {breed:25s} - {count:3d} images")

# Create image paths and labels
for img_file in image_files:
    breed = extract_breed(img_file)
    if breed:
        all_images.append(os.path.join(image_dir, img_file))
        all_labels.append(breed_to_idx[breed])

print(f"\nTotal images loaded: {len(all_images)}")
print(f"Total classes: {len(unique_breeds)}")


# Data Preprocessing and Augmentation
# Define transforms for training and validation
train_transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats
])

val_transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Split dataset into train and validation sets (80-20 split)
train_images, val_images, train_labels, val_labels = train_test_split(
    all_images, all_labels, test_size=0.2, random_state=42, stratify=all_labels
)

print(f"Training samples: {len(train_images)}")
print(f"Validation samples: {len(val_images)}")

# Create datasets
train_dataset = PetDataset(train_images, train_labels, transform=train_transform)
val_dataset = PetDataset(val_images, val_labels, transform=val_transform)

# Create data loaders
# IMPORTANT: num_workers=0 to avoid multiprocessing issues in Jupyter notebooks
# If you get pickling errors, make sure to re-run this cell!
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)

# Verify num_workers is 0
print(f"\nBatch size: {batch_size}")
print(f"Training batches: {len(train_loader)}")
print(f"Validation batches: {len(val_loader)}")
print(f"✓ DataLoader num_workers: {train_loader.num_workers} (should be 0)")
if train_loader.num_workers != 0:
    print("⚠ WARNING: num_workers is not 0! This will cause errors. Re-run this cell!")


# Visualize some sample images
def visualize_samples(dataset, num_samples=8):
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    axes = axes.ravel()
    
    indices = np.random.choice(len(dataset), num_samples, replace=False)
    
    for idx, ax in enumerate(axes):
        image, label = dataset[indices[idx]]
        
        # Denormalize for visualization
        image_np = image.numpy().transpose((1, 2, 0))
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        image_np = std * image_np + mean
        image_np = np.clip(image_np, 0, 1)
        
        ax.imshow(image_np)
        ax.set_title(f"Breed: {idx_to_breed[label]}", fontsize=10)
        ax.axis('off')
    
    plt.tight_layout()
    plt.show()

print("Sample training images:")
visualize_samples(train_dataset, num_samples=8)


# IMPROVED CNN Model Architecture with Transfer Learning
# Using a pre-trained ResNet18 as backbone for better performance
import torchvision.models as models
from torchvision.models import ResNet18_Weights

# Option 1: Use pre-trained ResNet18 (RECOMMENDED - much better results)
use_transfer_learning = True

if use_transfer_learning:
    # Load pre-trained ResNet18 (using modern API)
    model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)
    
    # Freeze early layers (optional - can unfreeze for fine-tuning)
    # for param in model.parameters():
    #     param.requires_grad = False
    
    # Replace the final fully connected layer for our 37 classes
    num_features = model.fc.in_features
    model.fc = nn.Sequential(
        nn.Dropout(0.5),
        nn.Linear(num_features, 256),
        nn.ReLU(inplace=True),
        nn.Dropout(0.5),
        nn.Linear(256, len(unique_breeds))
    )
    print("✓ Using pre-trained ResNet18 with transfer learning")
else:
    # Option 2: Custom CNN (original architecture)
    class PetBreedClassifier(nn.Module):
        def __init__(self, num_classes=37):
            super(PetBreedClassifier, self).__init__()
            
            # Feature extraction layers
            self.features = nn.Sequential(
                # First convolutional block
                nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
                
                # Second convolutional block
                nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=2, stride=2),
                
                # Third convolutional block
                nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=2, stride=2),
                
                # Fourth convolutional block
                nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=2, stride=2),
            )
            
            # Adaptive pooling and classifier
            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            self.classifier = nn.Sequential(
                nn.Dropout(0.5),
                nn.Linear(512, 256),
                nn.ReLU(inplace=True),
                nn.Dropout(0.5),
                nn.Linear(256, num_classes)
            )
        
        def forward(self, x):
            x = self.features(x)
            x = self.avgpool(x)
            x = torch.flatten(x, 1)
            x = self.classifier(x)
            return x
    
    # Initialize custom model (only if not using transfer learning)
    model = PetBreedClassifier(num_classes=len(unique_breeds)).to(device)

# Move model to device
model = model.to(device)

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"Model Architecture:")
print(f"Total parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")
print(f"\nModel structure:")
print(model)


# Training Configuration
# IMPORTANT: Before training, verify that train_loader.num_workers == 0
# If you see pickling errors, go back and re-run Cell 4 (Data Preprocessing)
print("Checking DataLoader configuration...")
print(f"train_loader.num_workers = {train_loader.num_workers}")
print(f"val_loader.num_workers = {val_loader.num_workers}")
if train_loader.num_workers != 0:
    print("\n⚠⚠⚠ ERROR: num_workers is not 0!")
    print("Please go back to Cell 4 and re-run it, then come back here.")
    raise ValueError("DataLoader num_workers must be 0 in Jupyter notebooks. Re-run Cell 4!")
print("✓ DataLoader configuration is correct.\n")

# IMPROVED HYPERPARAMETERS for better learning
criterion = nn.CrossEntropyLoss()

# Lower learning rate for better convergence
# If using transfer learning, use even lower LR for fine-tuning
if use_transfer_learning:
    learning_rate = 0.0001  # Lower LR for pre-trained model
    print("Using lower learning rate (0.0001) for transfer learning")
else:
    learning_rate = 0.0005  # Lower than before for custom model
    print("Using learning rate (0.0005) for custom model")

optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
# Use ReduceLROnPlateau scheduler - reduces LR when loss plateaus
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)

num_epochs = 15
best_val_acc = 0.0
train_losses = []
train_accs = []
val_losses = []
val_accs = []

print("Starting training...")
print(f"Number of epochs: {num_epochs}")
print(f"Initial learning rate: {optimizer.param_groups[0]['lr']}")
print("=" * 80)

# Training Loop
for epoch in range(num_epochs):
    # Training phase
    model.train()
    running_loss = 0.0
    correct_train = 0
    total_train = 0
    
    train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Train]")
    for images, labels in train_pbar:
        images, labels = images.to(device), labels.to(device)
        
        # Forward pass
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Statistics
        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total_train += labels.size(0)
        correct_train += (predicted == labels).sum().item()
        
        # Update progress bar
        train_pbar.set_postfix({
            'loss': f'{running_loss/(len(train_pbar)):.4f}',
            'acc': f'{100*correct_train/total_train:.2f}%'
        })
    
    train_loss = running_loss / len(train_loader)
    train_acc = 100 * correct_train / total_train
    
    # Validation phase
    model.eval()
    val_loss = 0.0
    correct_val = 0
    total_val = 0
    
    with torch.no_grad():
        val_pbar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Val]")
        for images, labels in val_pbar:
            images, labels = images.to(device), labels.to(device)
            
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            val_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total_val += labels.size(0)
            correct_val += (predicted == labels).sum().item()
            
            val_pbar.set_postfix({
                'loss': f'{val_loss/(len(val_pbar)):.4f}',
                'acc': f'{100*correct_val/total_val:.2f}%'
            })
    
    val_loss = val_loss / len(val_loader)
    val_acc = 100 * correct_val / total_val
    
    # Save best model
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), 'best_model.pth')
        print(f"\\n✓ New best model saved! Validation Accuracy: {val_acc:.2f}%")
    
    # Update learning rate (ReduceLROnPlateau uses validation loss)
    scheduler.step(val_loss)
    
    # Store metrics
    train_losses.append(train_loss)
    train_accs.append(train_acc)
    val_losses.append(val_loss)
    val_accs.append(val_acc)
    
    print(f"\\nEpoch {epoch+1}/{num_epochs} Summary:")
    print(f"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
    print(f"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%")
    print(f"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
    print("=" * 80)

print(f"\\nTraining completed! Best validation accuracy: {best_val_acc:.2f}%")

# Plot Training History
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

# Loss plot
ax1.plot(train_losses, label='Train Loss', linewidth=2)
ax1.plot(val_losses, label='Validation Loss', linewidth=2)
ax1.set_xlabel('Epoch', fontsize=12)
ax1.set_ylabel('Loss', fontsize=12)
ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
ax1.legend(fontsize=11)
ax1.grid(True, alpha=0.3)

# Accuracy plot
ax2.plot(train_accs, label='Train Accuracy', linewidth=2)
ax2.plot(val_accs, label='Validation Accuracy', linewidth=2)
ax2.set_xlabel('Epoch', fontsize=12)
ax2.set_ylabel('Accuracy (%)', fontsize=12)
ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
ax2.legend(fontsize=11)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"Final Training Accuracy: {train_accs[-1]:.2f}%")
print(f"Final Validation Accuracy: {val_accs[-1]:.2f}%")
print(f"Best Validation Accuracy: {best_val_acc:.2f}%")


# DIAGNOSTIC: Check if training completed properly
print("=" * 80)
print("TRAINING DIAGNOSTIC")
print("=" * 80)

if 'train_losses' in globals() and len(train_losses) > 0:
    print(f"✓ Training appears to have run")
    print(f"  - Epochs completed: {len(train_losses)}")
    print(f"  - Final training loss: {train_losses[-1]:.4f}")
    print(f"  - Final training accuracy: {train_accs[-1]:.2f}%")
    print(f"  - Final validation accuracy: {val_accs[-1]:.2f}%")
    print(f"  - Best validation accuracy: {best_val_acc:.2f}%")
    
    if len(train_losses) < 15:
        print(f"\n⚠ WARNING: Only {len(train_losses)} epochs completed (expected 15)")
        print("  Training was likely interrupted. Please re-run Cell 8.")
    
    if train_losses[-1] > 2.0:
        print(f"\n⚠ WARNING: Training loss is very high ({train_losses[-1]:.4f})")
        print("  Model may not have learned properly. Consider re-training.")
    
    if best_val_acc < 30:
        print(f"\n⚠ WARNING: Best validation accuracy is very low ({best_val_acc:.2f}%)")
        print("  This suggests training didn't work well.")
        print("  Possible causes:")
        print("    - Training was interrupted too early")
        print("    - Learning rate too high/low")
        print("    - Model needs more epochs")
        print("\n  RECOMMENDATION: Re-train with more epochs (20-25)")
else:
    print("✗ Training doesn't appear to have run at all!")
    print("  Please run Cell 8 (Training Loop) first.")
    print("=" * 80)


# Load best model and evaluate
model.load_state_dict(torch.load('best_model.pth'))
model.eval()

all_preds = []
all_labels = []

print("Evaluating on validation set...")
with torch.no_grad():
    for images, labels in tqdm(val_loader):
        images = images.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(labels.numpy())

# Calculate accuracy
accuracy = 100 * sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_labels)
print(f"\\nOverall Validation Accuracy: {accuracy:.2f}%")

# Classification Report
print("\\n" + "=" * 80)
print("CLASSIFICATION REPORT")
print("=" * 80)
target_names = [idx_to_breed[i] for i in range(len(unique_breeds))]
report = classification_report(all_labels, all_preds, target_names=target_names, digits=2)
print(report)

# Confusion Matrix
cm = confusion_matrix(all_labels, all_preds)

# Plot confusion matrix (subset for readability)
plt.figure(figsize=(20, 16))
sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', 
            xticklabels=[idx_to_breed[i] for i in range(len(unique_breeds))],
            yticklabels=[idx_to_breed[i] for i in range(len(unique_breeds))],
            cbar_kws={'label': 'Count'})
plt.title('Confusion Matrix - Pet Breed Classification', fontsize=16, fontweight='bold', pad=20)
plt.xlabel('Predicted Breed', fontsize=12)
plt.ylabel('True Breed', fontsize=12)
plt.xticks(rotation=45, ha='right', fontsize=8)
plt.yticks(rotation=0, fontsize=8)
plt.tight_layout()
plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

print("Confusion matrix saved as 'confusion_matrix.png'")

# Visualize Predictions on Sample Images
def visualize_predictions(model, dataset, num_samples=12):
    model.eval()
    fig, axes = plt.subplots(3, 4, figsize=(20, 15))
    axes = axes.ravel()
    
    indices = np.random.choice(len(dataset), num_samples, replace=False)
    
    with torch.no_grad():
        for idx, ax in enumerate(axes):
            image, true_label = dataset[indices[idx]]
            
            # Get prediction
            image_tensor = image.unsqueeze(0).to(device)
            output = model(image_tensor)
            _, predicted = torch.max(output, 1)
            predicted_label = predicted.item()
            confidence = torch.nn.functional.softmax(output, dim=1)[0][predicted_label].item()
            
            # Denormalize for visualization
            image_np = image.numpy().transpose((1, 2, 0))
            mean = np.array([0.485, 0.456, 0.406])
            std = np.array([0.229, 0.224, 0.225])
            image_np = std * image_np + mean
            image_np = np.clip(image_np, 0, 1)
            
            ax.imshow(image_np)
            
            # Color code: green if correct, red if wrong
            color = 'green' if predicted_label == true_label else 'red'
            title = f"True: {idx_to_breed[true_label]}\\n"
            title += f"Pred: {idx_to_breed[predicted_label]}\\n"
            title += f"Conf: {confidence*100:.1f}%"
            
            ax.set_title(title, fontsize=10, color=color, fontweight='bold')
            ax.axis('off')
    
    plt.tight_layout()
    plt.savefig('sample_predictions.png', dpi=300, bbox_inches='tight')
    plt.show()

print("Sample predictions on validation set:")
visualize_predictions(model, val_dataset, num_samples=12)


# Per-class Accuracy Analysis
class_accuracies = {}
for i in range(len(unique_breeds)):
    class_mask = np.array(all_labels) == i
    if class_mask.sum() > 0:
        class_correct = sum(np.array(all_preds)[class_mask] == i)
        class_total = class_mask.sum()
        class_accuracies[idx_to_breed[i]] = 100 * class_correct / class_total

# Sort by accuracy
sorted_classes = sorted(class_accuracies.items(), key=lambda x: x[1], reverse=True)

print("\\n" + "=" * 80)
print("PER-CLASS ACCURACY (Top 10 and Bottom 10)")
print("=" * 80)
print("\\nTop 10 Classes:")
for breed, acc in sorted_classes[:10]:
    print(f"  {breed:25s}: {acc:6.2f}%")

print("\\nBottom 10 Classes:")
for breed, acc in sorted_classes[-10:]:
    print(f"  {breed:25s}: {acc:6.2f}%")

# Visualize per-class accuracy
plt.figure(figsize=(15, 10))
breeds_sorted = [x[0] for x in sorted_classes]
accs_sorted = [x[1] for x in sorted_classes]

colors = ['green' if acc >= 80 else 'orange' if acc >= 60 else 'red' for acc in accs_sorted]
plt.barh(range(len(breeds_sorted)), accs_sorted, color=colors)
plt.yticks(range(len(breeds_sorted)), breeds_sorted)
plt.xlabel('Accuracy (%)', fontsize=12)
plt.ylabel('Breed', fontsize=12)
plt.title('Per-Class Accuracy - Pet Breed Classification', fontsize=14, fontweight='bold')
plt.axvline(x=80, color='green', linestyle='--', alpha=0.5, label='80% threshold')
plt.axvline(x=60, color='orange', linestyle='--', alpha=0.5, label='60% threshold')
plt.legend()
plt.tight_layout()
plt.savefig('per_class_accuracy.png', dpi=300, bbox_inches='tight')
plt.show()
